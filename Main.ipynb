{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUqVEc8ma9rD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAFc5h2pz3JL"
      },
      "outputs": [],
      "source": [
        "!pip install pynrrd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW-JyhXJ0OFZ"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4scTPxdZAUT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPSdoOCPxgF8"
      },
      "outputs": [],
      "source": [
        "clinical_features = pd.read_excel(\"/content/drive/MyDrive/GNN_/phenotypic_information.xlsx\")\n",
        "label = pd.read_excel(\"/content/drive/MyDrive/GNN_/Label.xlsx\")\n",
        "clinical_features[\"label\"] = label['Staging(Metastasis)#(Mx -replaced by -1)[M]']\n",
        "clinical_features.to_excel(\"/content/drive/MyDrive/GNN_/phenotypic_information.xlsx\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ILSJ8kyRkXx"
      },
      "outputs": [],
      "source": [
        "dataa = pd.read_excel(\"/content/drive/MyDrive/GNN_/phenotypic_information.xlsx\")\n",
        "dataa.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6MgePjXSESL"
      },
      "outputs": [],
      "source": [
        "dataa[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSpEgGNsiW_X"
      },
      "outputs": [],
      "source": [
        "np.unique(clinical_features[\"label\"], return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8pQAFKGrICM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch_geometric.data import DataLoader, Data\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import nrrd\n",
        "from scipy.ndimage import zoom\n",
        "import glob\n",
        "class VolumetricGraphDataset(Dataset):\n",
        "    def __init__(self, root, feature_file, classes=[\"0\", \"1\",\"2\"], transform=None):\n",
        "        self.classes = classes\n",
        "        self.root = root\n",
        "        self.data_list = []\n",
        "\n",
        "        self.additional_features = pd.read_excel(feature_file)\n",
        "        self.name_ids = self.additional_features['Image Data ID']\n",
        "        self.feature_scaler = StandardScaler()\n",
        "        feature_columns = [col for col in self.additional_features.columns if col not in ['Image Data ID', 'label']]\n",
        "\n",
        "        self.additional_features[feature_columns] = self.feature_scaler.fit_transform(\n",
        "            self.additional_features[feature_columns]\n",
        "        )\n",
        "\n",
        "        self.process()\n",
        "\n",
        "    def volume_to_graph(self, volume_path, additional_features):\n",
        "        volume = nrrd.read(volume_path)[0]\n",
        "        target_size = (32, 32, 32)\n",
        "        volume = resize_volume(volume, target_size)\n",
        "\n",
        "        # Normalize volume\n",
        "        volume_min = volume.min()\n",
        "        volume_max = volume.max()\n",
        "        if volume_max > volume_min:  # Avoid division by zero\n",
        "            volume = (volume - volume_min) / (volume_max - volume_min)\n",
        "\n",
        "        nodes = []\n",
        "        edges = []\n",
        "        node_features = []\n",
        "\n",
        "        for i in range(volume.shape[0]):\n",
        "            for j in range(volume.shape[1]):\n",
        "                for k in range(volume.shape[2]):\n",
        "                    nodes.append((i, j, k))\n",
        "                    node_feat = [volume[i, j, k]] + additional_features.tolist()\n",
        "                    node_features.append(node_feat)\n",
        "\n",
        "        threshold = 0.5\n",
        "        for idx, (i, j, k) in enumerate(nodes):\n",
        "            for di, dj, dk in [(-1, 0, 0), (1, 0, 0), (0, -1, 0), (0, 1, 0), (0, 0, -1), (0, 0, 1)]:\n",
        "                ni, nj, nk = i + di, j + dj, k + dk\n",
        "                if (0 <= ni < volume.shape[0] and 0 <= nj < volume.shape[1] and 0 <= nk < volume.shape[2]):\n",
        "                    neighbor_idx = (ni * volume.shape[1] * volume.shape[2] + nj * volume.shape[2] + nk)\n",
        "                    if abs(volume[i,j,k] - volume[ni,nj,nk]) < threshold:\n",
        "                        edges.append([idx, neighbor_idx])\n",
        "\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "        x = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "        return x, edge_index\n",
        "\n",
        "    def process(self):\n",
        "        for name_idx, name_id in enumerate(self.name_ids):\n",
        "            class_dir = os.path.join(self.root, str(name_id))\n",
        "\n",
        "            # for vol_name in os.listdir(class_dir):\n",
        "            vol_path = glob.glob(class_dir + \"/*\")\n",
        "            print(f'vol_path {vol_path}' )\n",
        "            vol_path=vol_path[0]\n",
        "            vol_features = self.additional_features[\n",
        "                self.additional_features['Image Data ID'] == name_id\n",
        "            ]\n",
        "\n",
        "            if len(vol_features) > 0:\n",
        "                feature_columns = [col for col in vol_features.columns if col not in ['Image Data ID', 'label']]\n",
        "                additional_features = vol_features[feature_columns].values[0]\n",
        "                label = vol_features['label'].values[0]\n",
        "                x, edge_index = self.volume_to_graph(vol_path, additional_features)\n",
        "                data = Data(x=x,\n",
        "                            edge_index=edge_index,\n",
        "                            y=torch.tensor([label]))\n",
        "                self.data_list.append(data)\n",
        "\n",
        "    def __len__(self):  # Corrected to __len__\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):  # Corrected to __getitem__\n",
        "        return self.data_list[idx]\n",
        "\n",
        "def resize_volume(volume, target_shape):\n",
        "    current_shape = volume.shape\n",
        "    factors = [float(t) / float(s) for t, s in zip(target_shape, current_shape)]\n",
        "    return zoom(volume, factors, order=1)\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GINConv, global_mean_pool\n",
        "\n",
        "def make_convolution(in_channels, out_channels):\n",
        "    return GINConv(nn.Sequential(\n",
        "        nn.Linear(in_channels, out_channels),\n",
        "        nn.BatchNorm1d(out_channels),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(out_channels, out_channels),\n",
        "        nn.BatchNorm1d(out_channels),\n",
        "        nn.ReLU()\n",
        "    ))\n",
        "\n",
        "class GINClassification(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
        "        super(GINClassification, self).__init__()\n",
        "        self.conv1 = make_convolution(in_channels, hidden_channels)\n",
        "        self.conv2 = make_convolution(hidden_channels, hidden_channels)\n",
        "        self.conv3 = make_convolution(hidden_channels, out_channels)\n",
        "        self.classifier = nn.Linear(out_channels, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = global_mean_pool(x, batch=batch)\n",
        "        return self.classifier(x)\n",
        "\n",
        "    def extract_embedding(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = global_mean_pool(x, batch=batch)\n",
        "        return x\n",
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "import itertools\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, test_loader, optimizer, criterion,fold, num_epochs=500, model_save_path='models/'):\n",
        "    best_model_path = os.path.join(model_save_path, 'best_model.pth')\n",
        "    last_model_path = os.path.join(model_save_path, 'last_model.pth')\n",
        "    best_test_accuracy = 0.0\n",
        "    best_f1 = 0.0\n",
        "\n",
        "    # Initialize log data\n",
        "    log_data = {\n",
        "        'Epoch': [],\n",
        "        'Train Average Loss': [],\n",
        "        'Test Average Loss': [],\n",
        "        'Train Accuracy': [],  # Add this line\n",
        "        'Test Accuracy': [],\n",
        "        'Precision': [],\n",
        "        'Recall': [],\n",
        "        'F1 Score': [],\n",
        "        'Confusion Matrix': []\n",
        "    }\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs), desc='Training'):\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "        correct_train = 0  # Initialize correct predictions for training\n",
        "        total_train = 0  # Initialize total samples for training\n",
        "\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        for data in train_loader:\n",
        "            x, edge_index, y = data.x, data.edge_index, data.y\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x, edge_index, data.batch)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * data.num_graphs\n",
        "            total_samples += data.num_graphs\n",
        "\n",
        "            _, predicted = torch.max(out, 1)\n",
        "            correct_train += (predicted == y).sum().item()\n",
        "            total_train += y.size(0)\n",
        "\n",
        "        avg_loss = total_loss / total_samples\n",
        "        train_accuracy = correct_train / total_train  # Calculate train accuracy\n",
        "\n",
        "        # Test loop\n",
        "        model.eval()\n",
        "        test_total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        true_labels = []\n",
        "        predicted_labels = []\n",
        "        with torch.no_grad():\n",
        "            for data in test_loader:\n",
        "                x, edge_index, y = data.x, data.edge_index, data.y\n",
        "                out = model(x, edge_index, data.batch)\n",
        "                loss = criterion(out, y)\n",
        "                test_total_loss += loss.item() * data.num_graphs\n",
        "                _, predicted = torch.max(out, 1)\n",
        "                total += y.size(0)\n",
        "                correct += (predicted == y).sum().item()\n",
        "                true_labels.extend(y.numpy())\n",
        "                predicted_labels.extend(predicted.numpy())\n",
        "\n",
        "        test_accuracy = correct / total\n",
        "        test_avg_loss = test_total_loss / total\n",
        "        f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "        precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "        recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "        cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "        tqdm.write(f'Epoch [{epoch + 1}/{num_epochs}], Train Avg Loss: {avg_loss:.5f}, Train Accuracy: {train_accuracy:.5f}, '\n",
        "                    f'Test Avg Loss: {test_avg_loss:.5f}, Test Accuracy: {test_accuracy:.5f}, '\n",
        "                    f'Precision: {precision:.5f}, Recall: {recall:.5f}, F1 Score: {f1:.5f}')\n",
        "\n",
        "        # Log data\n",
        "        log_data['Epoch'].append(epoch + 1)\n",
        "        log_data['Train Average Loss'].append(avg_loss)\n",
        "        log_data['Test Average Loss'].append(test_avg_loss)\n",
        "        log_data['Train Accuracy'].append(train_accuracy)  # Log train accuracy\n",
        "        log_data['Test Accuracy'].append(test_accuracy)\n",
        "        log_data['Precision'].append(precision)\n",
        "        log_data['Recall'].append(recall)\n",
        "        log_data['F1 Score'].append(f1)\n",
        "        log_data['Confusion Matrix'].append(cm.tolist())\n",
        "\n",
        "        # Save the best model based on test accuracy and F1-score\n",
        "        if test_accuracy > best_test_accuracy or (test_accuracy == best_test_accuracy and best_f1 < f1):\n",
        "            tqdm.write(\"$$$ best model is updated according to accuracy! $$$\")\n",
        "            best_test_accuracy = test_accuracy\n",
        "            best_f1 = f1\n",
        "            os.makedirs(model_save_path, exist_ok=True)\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "    # Save the last model after training\n",
        "    os.makedirs(model_save_path, exist_ok=True)\n",
        "    torch.save(model.state_dict(), last_model_path)\n",
        "\n",
        "    # Load the best model for evaluation\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "    # Convert log_data to DataFrame\n",
        "    log_df = pd.DataFrame(log_data)\n",
        "    log_df.to_csv(os.path.join(model_save_path, f'train_log_{fold}.csv'), index=False)\n",
        "\n",
        "    # Plotting Loss and Accuracy\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot training and test loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(log_data['Epoch'], log_data['Train Average Loss'], label='Train Loss', color='blue')\n",
        "    plt.plot(log_data['Epoch'], log_data['Test Average Loss'], label='Test Loss', color='red')\n",
        "    plt.title('Training and Test Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot training and test accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(log_data['Epoch'], log_data['Train Accuracy'], label='Train Accuracy', color='blue')\n",
        "    plt.plot(log_data['Epoch'], log_data['Test Accuracy'], label='Test Accuracy', color='green')\n",
        "    plt.title('Train and Test Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Overall metrics\n",
        "    overall_precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "    overall_recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "    overall_f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "    print(f'Overall Test Accuracy: {test_accuracy:.5f}, Precision: {overall_precision:.5f}, Recall: {overall_recall:.5f}, F1 Score: {overall_f1:.5f}')\n",
        "\n",
        "    # Plotting the confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = range(len(set(true_labels)))\n",
        "    plt.xticks(tick_marks, tick_marks)\n",
        "    plt.yticks(tick_marks, tick_marks)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def test_model(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data.to(device)  # Move data to device\n",
        "\n",
        "            output = model(data)\n",
        "            pred = output.max(dim=1)[1]\n",
        "\n",
        "            correct += pred.eq(data.y).sum().item()\n",
        "            total += data.y.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHkAXGcGHSEn"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/breast_MRI_DUKE_image/breast MRI cropped/IHS_fuse/fused_data\"\n",
        "feature_file = \"/content/drive/MyDrive/GNN_/phenotypic_information.xlsx\"\n",
        "\n",
        "dataset = VolumetricGraphDataset(data_dir, feature_file)\n",
        "print( dataset)\n",
        "num_node_features = dataset[0].x.size(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybOutLE7hcg3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "labels = [data.y.item() for data in dataset]\n",
        "unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "print(\"Class distribution before oversampling:\")\n",
        "for lbl, count in zip(unique_labels, counts):\n",
        "    print(f\"Class {lbl}: {count} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0O6Rg_hhiBE"
      },
      "outputs": [],
      "source": [
        "oversampler = SMOTE()\n",
        "node_features = np.vstack([data.x.numpy() for data in dataset])\n",
        "node_labels = np.array(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pC1hlNcZyAW"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "graph_features = []\n",
        "graph_labels = []\n",
        "\n",
        "for data in dataset:\n",
        "    graph_features.append(data.x.mean(dim=0).numpy())\n",
        "    graph_labels.append(data.y.item())\n",
        "\n",
        "graph_features = np.array(graph_features)\n",
        "graph_labels = np.array(graph_labels)\n",
        "class_counts = Counter(graph_labels)\n",
        "\n",
        "valid_indices = [i for i, label in enumerate(graph_labels) if class_counts[label] > 3]\n",
        "graph_features = graph_features[valid_indices]\n",
        "graph_labels = graph_labels[valid_indices]\n",
        "\n",
        "oversampled_features, oversampled_labels = oversampler.fit_resample(graph_features, graph_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pojfnf0_alWm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "unique_labels = np.unique(graph_labels)\n",
        "\n",
        "label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}\n",
        "\n",
        "graph_labels = np.array([label_mapping[label] for label in graph_labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvBEagJgauYx"
      },
      "outputs": [],
      "source": [
        "print(\"Class distribution after remapping:\")\n",
        "unique_labels, counts = np.unique(graph_labels, return_counts=True)\n",
        "for lbl, count in zip(unique_labels, counts):\n",
        "    print(f\"Class {lbl}: {count} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A58ooCcda5FZ"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "oversampler = SMOTE(k_neighbors=1)\n",
        "oversampled_features, oversampled_labels = oversampler.fit_resample(graph_features, graph_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ4P7qTqhmej"
      },
      "outputs": [],
      "source": [
        "print(\"Class distribution after oversampling:\")\n",
        "unique_labels, counts = np.unique(oversampled_labels, return_counts=True)\n",
        "for lbl, count in zip(unique_labels, counts):\n",
        "    print(f\"Class {lbl}: {count} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKZMNOqOdFmM"
      },
      "outputs": [],
      "source": [
        "oversampled_data_list = []\n",
        "for i in range(len(oversampled_labels)):\n",
        "    additional_features = torch.tensor(oversampled_features[i], dtype=torch.float).unsqueeze(0)\n",
        "    label = torch.tensor([oversampled_labels[i]], dtype=torch.long)\n",
        "\n",
        "    x = additional_features.repeat(10, 1)\n",
        "    edge_index = torch.tensor([[i, j] for i in range(10) for j in range(10)], dtype=torch.long).t().contiguous()\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, y=label)\n",
        "    oversampled_data_list.append(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMcAMa-Pm1xt"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report\n",
        "from torch_geometric.data import DataLoader\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_node_features = 16\n",
        "num_classes = 3\n",
        "num_epochs = 150\n",
        "k_folds = 5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "fold_train_losses = []\n",
        "fold_val_losses = []\n",
        "fold_train_accuracies = []\n",
        "fold_val_accuracies = []\n",
        "\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(oversampled_data_list)):\n",
        "    print(f\"Fold {fold + 1}/5\")\n",
        "    train_data = [oversampled_data_list[i] for i in train_idx]\n",
        "    val_data = [oversampled_data_list[i] for i in val_idx]\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "    test_loader = DataLoader(val_data, shuffle=False)\n",
        "    model = GINClassification(in_channels=num_node_features, hidden_channels=1000, out_channels=100, num_classes=num_classes)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    train_model(model, train_loader, test_loader, optimizer, criterion,fold=fold, num_epochs=num_epochs, model_save_path=\"/content/drive/MyDrive/GNN_/Staging(Metastasis)#(Mx -replaced by -1)[M]/fusion/model\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}